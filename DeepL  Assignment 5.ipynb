{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPP4FJsvG+MfIz2ttQU29rR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyAPYAdKA12R"},"outputs":[],"source":["def Psat(self, T):\n","    pop= self.getPborder(T)\n","    boolean=int(pop[0])\n","\n","    P1=pop[1]\n","    P2=pop[2]\n","    if boolean:\n","        Pmin = float(min([P1, P2]))\n","        Pmax = float(max([P1, P2]))\n","        Tr=T/self.typeMolecule.Tc\n","        w=0.5*(1+scipy.tanh((10**5)*(Tr-0.6)))\n","        fi1=0.5*(1-scipy.tanh(8*((Tr**0.4)-1)))\n","        fi2=0.460*scipy.sqrt(1-(Tr-0.566)**2/(0.434**2)+0.494)\n","\n","        guess = Pmin+(Pmax-Pmin)*((1-w**2)*fi1+(w**2)*fi2)   # error here\n","\n","        solution = scipy.optimize.newton(funcPsat,guess, args=(T,self))"]},{"cell_type":"code","source":["import numpy as np\n","import keras.backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, Lambda\n","#from keras.utils import np_utils\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","import gensim\n","data=open('corona.txt','r')\n","corona_data = [text for text in data if text.count(' ') >= 2]\n","vectorize = Tokenizer()\n","vectorize.fit_on_texts(corona_data)\n","corona_data = vectorize.texts_to_sequences(corona_data)\n","total_vocab = sum(len(s) for s in corona_data)\n","word_count = len(vectorize.word_index) + 1\n","window_size = 2"],"metadata":{"id":"sPcHruOHhH64"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cbow_model(data, window_size, total_vocab):\n","    total_length = window_size*2\n","    for text in data:\n","        text_len = len(text)\n","        for idx, word in enumerate(text):\n","            context_word = []\n","            target   = []\n","            begin = idx - window_size\n","            end = idx + window_size + 1\n","            context_word.append([text[i] for i in range(begin, end) if 0 <= i < text_len and i != idx])\n","            target.append(word)\n","            contextual = sequence.pad_sequences(context_word, total_length=total_length)\n","            final_target = np_utils.to_categorical(target, total_vocab)\n","            yield(contextual, final_target)"],"metadata":{"id":"sXA1lpBOiKKR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#CBOW On sampel data"],"metadata":{"id":"G2yC9RXCiXc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Sequential()\n","model.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\n","model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\n","model.add(Dense(total_vocab, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","for i in range(10):\n","    cost = 0\n","    for x, y in cbow_model(data, window_size, total_vocab):\n","        cost += model.train_on_batch(contextual, final_target)\n","    print(i, cost)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DqnagAn3iUTg","executionInfo":{"status":"ok","timestamp":1697108288681,"user_tz":-330,"elapsed":1546,"user":{"displayName":"Aditya Nanaware","userId":"04026375241103718273"}},"outputId":"29b7109c-f295-4069-b9d3-ede88c091182"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0\n","1 0\n","2 0\n","3 0\n","4 0\n","5 0\n","6 0\n","7 0\n","8 0\n","9 0\n"]}]},{"cell_type":"code","source":["#CBOW"],"metadata":{"id":"gckq7hdSiedx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dimensions=100\n","vect_file = open('vectors.txt' ,'w')\n","vect_file.write('{} {}\\n'.format(total_vocab,dimensions))\n","#Next, we will access the weights of the trained model and write it to the above created file.\n","\n","weights = model.get_weights()[0]\n","for text, i in vectorize.word_index.items():\n","    final_vec = ' '.join(map(str, list(weights[i, :])))\n","    vect_file.write('{} {}\\n'.format(text, final_vec))\n","vect_file.close()"],"metadata":{"id":"7AjjKvCvimHp"},"execution_count":null,"outputs":[]}]}