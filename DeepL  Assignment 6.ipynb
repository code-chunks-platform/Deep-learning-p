{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"kymKPWIS9QHA"},"source":["from keras.layers import Input, Lambda, Dense, Flatten\n","from keras.models import Model\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg16 import preprocess_input\n","from keras.preprocessing import image\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","import numpy as np\n","from glob import glob\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ztvnQbfNL13G"},"source":["IMAGE_SIZE = [224, 224]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UyDyxaIHk0vC","outputId":"68e1aa97-b6ce-4c9a-f55e-4d1bd086d059","executionInfo":{"status":"ok","timestamp":1697391043983,"user_tz":-330,"elapsed":23146,"user":{"displayName":"Aditya Nanaware","userId":"04026375241103718273"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z72pGopElNKj","outputId":"f3aa501c-c6a6-4f34-b073-0d5df1548c02","executionInfo":{"status":"ok","timestamp":1697391047556,"user_tz":-330,"elapsed":698,"user":{"displayName":"Aditya Nanaware","userId":"04026375241103718273"}}},"source":["!ls '/content/drive'\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MyDrive\n"]}]},{"cell_type":"code","metadata":{"id":"79xfCi1VL4WE"},"source":["#Give dataset path\n","train_path = '/train'\n","test_path = '/test'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":262},"id":"nXzCeUIa4APa","outputId":"bd65ee17-a0e9-4ade-ccca-3c03ef0b4be4","executionInfo":{"status":"error","timestamp":1697391793793,"user_tz":-330,"elapsed":750,"user":{"displayName":"Aditya Nanaware","userId":"04026375241103718273"}}},"source":["from PIL import Image\n","import os\n","from IPython.display import display\n","from IPython.display import Image as _Imgdis\n","# creating a object\n","\n","\n","folder = train_path+'/train/benign'\n","\n","\n","onlybenignfiles = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n","print(\"Working with {0} images\".format(len(onlybenignfiles)))\n","print(\"Image examples: \")\n","\n","\n","for i in range(10):\n","    print(onlybenignfiles[i])\n","    display(_Imgdis(filename=folder + \"/\" + onlybenignfiles[i], width=240, height=240))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-6fee6e3af3ee>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0monlybenignfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Working with {0} images\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monlybenignfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image examples: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/trainhttps://drive.google.com/drive/u/0/folders/13u7X2DEoka9pGPJjStC1R_6KMIielTSs/train/benign'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDHko5V6MaVr","outputId":"abd2b981-eb27-44d7-d1ee-48dae37bb904","executionInfo":{"status":"ok","timestamp":1697391316214,"user_tz":-330,"elapsed":1520,"user":{"displayName":"Aditya Nanaware","userId":"04026375241103718273"}}},"source":["vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"Lt7P2RKid8PU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d6f25005-f44b-49a6-8397-ee6268b89b71","executionInfo":{"status":"ok","timestamp":1697391319107,"user_tz":-330,"elapsed":718,"user":{"displayName":"Aditya Nanaware","userId":"04026375241103718273"}}},"source":["vgg.input"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_1')>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"cwdsXOLlMdiR"},"source":["for layer in vgg.layers:\n","  layer.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jR_L6S1_lr4L","outputId":"6cf48c70-ccfe-463d-8ba2-087042c3677b","executionInfo":{"status":"ok","timestamp":1697391297084,"user_tz":-330,"elapsed":481,"user":{"displayName":"Aditya Nanaware","userId":"04026375241103718273"}}},"source":["folders = glob('/content/drive/MyDrive/skincancerdataset/train/*')\n","print(len(folders))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G56cRtmvMiVj","outputId":"8fb99889-e22b-40c4-f500-0311cc06eb6d","executionInfo":{"status":"ok","timestamp":1697391338560,"user_tz":-330,"elapsed":1030,"user":{"displayName":"Aditya Nanaware","userId":"04026375241103718273"}}},"source":["x = Flatten()(vgg.output)\n","prediction = Dense(len(folders), activation='softmax')(x)\n","model = Model(inputs=vgg.input, outputs=prediction)\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," dense (Dense)               (None, 0)                 0         \n","                                                                 \n","=================================================================\n","Total params: 14714688 (56.13 MB)\n","Trainable params: 0 (0.00 Byte)\n","Non-trainable params: 14714688 (56.13 MB)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"L6x9IHTBM1CA"},"source":["from keras import optimizers\n","\n","adam = optimizers.Adam()\n","model.compile(loss='binary_crossentropy',\n","              optimizer=adam,\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0YRD6xfnnrk"},"source":["train_datagen = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BPfA9FCGoHBF"},"source":["test_datagen = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"v-c2pnSGoJlL","outputId":"98f158af-ad8a-448f-ed9b-23972958f83a","executionInfo":{"status":"error","timestamp":1697391354879,"user_tz":-330,"elapsed":815,"user":{"displayName":"Aditya Nanaware","userId":"04026375241103718273"}}},"source":["train_set = train_datagen.flow_from_directory(train_path,\n","                                                 target_size = (224, 224),\n","                                                 batch_size = 32,\n","                                                 class_mode = 'categorical')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-a0cd322475d2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_set = train_datagen.flow_from_directory(train_path,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                  \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                  \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                  class_mode = 'categorical')\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \"\"\"\n\u001b[0;32m-> 1648\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/train'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nNPc_W70oLzY","outputId":"098d33de-ce4b-4349-eb5a-4906765b56b9"},"source":["test_set = test_datagen.flow_from_directory(test_path,\n","                                            target_size = (224, 224),\n","                                            batch_size = 32,\n","                                            class_mode = 'categorical')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 660 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jPaek3VoRUz","outputId":"736aac45-ce77-4ebf-c3dd-ef8e877019a2"},"source":["from datetime import datetime\n","from keras.callbacks import ModelCheckpoint\n","\n","\n","\n","checkpoint = ModelCheckpoint(filepath='mymodel.h5',\n","                               verbose=2, save_best_only=True)\n","\n","callbacks = [checkpoint]\n","\n","start = datetime.now()\n","\n","model_history=model.fit_generator(\n","  train_set,\n","  validation_data=test_set,\n","  epochs=10,\n","  steps_per_epoch=5,\n","  validation_steps=32,\n","    callbacks=callbacks ,verbose=2)\n","\n","\n","duration = datetime.now() - start\n","print(\"Training completed in time: \", duration)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","5/5 [==============================] - ETA: 0s - loss: 1.4748 - accuracy: 0.8188 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n","5/5 [==============================] - 423s 101s/step - loss: 1.4748 - accuracy: 0.8188 - val_loss: 1.6955 - val_accuracy: 0.7985\n","\n","Epoch 00001: val_loss improved from inf to 1.69549, saving model to mymodel.h5\n","Epoch 2/10\n","5/5 [==============================] - 83s 16s/step - loss: 1.4576 - accuracy: 0.8687\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n","Epoch 3/10\n","5/5 [==============================] - 83s 16s/step - loss: 1.7037 - accuracy: 0.7437\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n","Epoch 4/10\n","5/5 [==============================] - 76s 15s/step - loss: 1.7391 - accuracy: 0.8227\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n","Epoch 5/10\n","5/5 [==============================] - 82s 16s/step - loss: 1.4981 - accuracy: 0.7875\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n","Epoch 6/10\n","5/5 [==============================] - 82s 16s/step - loss: 2.3564 - accuracy: 0.7812\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n","Epoch 7/10\n","5/5 [==============================] - 82s 16s/step - loss: 2.2264 - accuracy: 0.7437\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n","Epoch 8/10\n","5/5 [==============================] - 83s 16s/step - loss: 1.9619 - accuracy: 0.8250\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n","Epoch 9/10\n","5/5 [==============================] - 83s 16s/step - loss: 2.1874 - accuracy: 0.8000\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n","Epoch 10/10\n","5/5 [==============================] - 72s 16s/step - loss: 1.2264 - accuracy: 0.8369\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n","Training completed in time:  0:19:11.106052\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"-xWwh8bVoYn9","outputId":"970ff35b-0446-42ba-d02f-f678d579f078","executionInfo":{"status":"error","timestamp":1697391391450,"user_tz":-330,"elapsed":566,"user":{"displayName":"Aditya Nanaware","userId":"04026375241103718273"}}},"source":["_# Plot training & validation loss values\n","plt.plot(model_history.history['accuracy'])\n","plt.plot(model_history.history['val_accuracy'])\n","plt.title('CNN Model accuracy values')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-c90d53667c3d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m_\u001b[0m\u001b[0;31m# Plot training & validation loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CNN Model accuracy values'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model_history' is not defined"]}]},{"cell_type":"code","metadata":{"id":"SjF3_pGFxaf0"},"source":[],"execution_count":null,"outputs":[]}]}